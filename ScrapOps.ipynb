{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMJ7Hg7gXscg27maqU5GFjz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditisingh2912/ScapOps/blob/main/ScrapOps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fnuv2uEeDyiv"
      },
      "outputs": [],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "WZs5GdlFFzhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "url = \"https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/\"\n",
        "response = requests.get(url,verify = False)\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    article = soup.find('div', class_='td-post-content tagdiv-type')\n",
        "\n",
        "    if article:\n",
        "        article_text = article.get_text()\n",
        "        print(article_text)\n",
        "    else:\n",
        "        print(\"Article content not found on the page.\")\n",
        "else:\n",
        "    print(\"Failed to retrieve the page. Status code:\", response.status_code)\n"
      ],
      "metadata": {
        "id": "PJUyG2HvF9Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url=\"https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/\"\n"
      ],
      "metadata": {
        "id": "g1SHihL_GCvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    article_elements = soup.find_all(class_='td-post-content tagdiv-type')\n",
        "\n",
        "    headerels = soup.find_all(class_=\"td-parallax-header\")\n",
        "    for h in headerels:\n",
        "        el = h.find_all(class_ = 'td-post-title')\n",
        "    for e in el:\n",
        "        extracted_text = e.find(class_=\"entry-title\").get_text()\n",
        "\n",
        "\n",
        "    for element in article_elements:\n",
        "        preformatted_elements = element.find_all(class_='wp-block-preformatted')\n",
        "        for preformatted in preformatted_elements:\n",
        "            preformatted.decompose()\n",
        "\n",
        "\n",
        "        extracted_text += element.get_text() + \"\\n\"\n",
        "\n",
        "    print(extracted_text)\n",
        "else:\n",
        "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "Zs3mTopEGSz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"output.txt\", \"a\", encoding=\"utf-8\")\n",
        "print(extracted_text, file=file)"
      ],
      "metadata": {
        "id": "oLZAk1bwGWzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrapearticle(url_id, url):\n",
        "    current_directory = os.getcwd()\n",
        "    directory = os.path.join(current_directory, \"Articles\")\n",
        "    file_path = os.path.join(directory, url_id + \".txt\")\n",
        "\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the page\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        article_elements = soup.find_all(class_='td-post-content tagdiv-type')\n",
        "\n",
        "        # Extracting title of the article\n",
        "        headerels = soup.find_all(class_=\"td-parallax-header\")\n",
        "        el = None  # Initialize el variable\n",
        "        for h in headerels:\n",
        "            el = h.find_all(class_='td-post-title')\n",
        "        if el:  # Check if el is not None\n",
        "            extracted_title = el[0].find(class_=\"entry-title\").get_text()\n",
        "        else:\n",
        "            extracted_title = \"\"  # Set default value if no title is found\n",
        "\n",
        "        # Extracting article text\n",
        "        extracted_text = \"\"\n",
        "        for element in article_elements:\n",
        "            preformatted_elements = element.find_all(class_='wp-block-preformatted')\n",
        "            for preformatted in preformatted_elements:\n",
        "                preformatted.decompose()\n",
        "            extracted_text += element.get_text() + \"\\n\"\n",
        "\n",
        "        # Write the extracted text to a file\n",
        "        with open(file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "            file.write(extracted_text)\n",
        "\n",
        "        # Return the extracted text and URL\n",
        "        return extracted_text, url\n",
        "    else:\n",
        "        print(f\"Failed to retrieve the page. Status code: {response.status_code}, URL_ID: {url_id}\")\n"
      ],
      "metadata": {
        "id": "JiDN811JGfK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_stopwords(path):\n",
        "    file = open(path, 'r',encoding='latin-1')\n",
        "    lines = file.readlines()\n",
        "    stopwords = [line.strip().split()[0] for line in lines]\n",
        "    return stopwords\n"
      ],
      "metadata": {
        "id": "gIIu-mZIGu9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_words(path,stopwords):\n",
        "    with open(path, 'r', encoding='latin-1') as file:\n",
        "        stopwords = [line.strip().split()[0] for line in file]\n",
        "    return stopwords"
      ],
      "metadata": {
        "id": "wouiARx3GwU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "W027WudTJG0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir()"
      ],
      "metadata": {
        "id": "PpZNAprVJi1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "current_directory = os.getcwd()\n",
        "folder_name = \"StopWords\"\n",
        "stopwords = []\n",
        "folder_path = os.path.join(current_directory, folder_name)\n",
        "print(folder_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "w2e9wO9eGzzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
        "    files = os.listdir(folder_path)\n",
        "    print(files)\n"
      ],
      "metadata": {
        "id": "rorbm9KWUX0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file_name in files:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        print(file_path)\n"
      ],
      "metadata": {
        "id": "bMSJC8pMUcY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file_name in files:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        if os.path.isfile(file_path):\n",
        "            stopwords = stopwords + extract_stopwords(file_path)\n",
        "            print(f\"File found: {file_name}\")"
      ],
      "metadata": {
        "id": "wJDd2cuZUi1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords"
      ],
      "metadata": {
        "id": "eY5yOmZwPOiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords)\n"
      ],
      "metadata": {
        "id": "ruKIaukvPZqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"stopwords.txt\"\n",
        "with open(file_path, 'w') as file:\n",
        "    for word in stopwords:\n",
        "        file.write(word + '\\n')\n",
        "\n",
        "print(\"stopwords list saved to:\", file_path)"
      ],
      "metadata": {
        "id": "V7Quw3rzHEst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "current_directory = os.getcwd()\n",
        "folder_name = \"MasterDictionary\"\n",
        "folder_path = os.path.join(current_directory, folder_name)\n",
        "if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
        "    files = os.listdir(folder_path)\n",
        "    for file_name in files:\n",
        "\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        if file_name[:8] == \"positive\":\n",
        "            pos = extract_words(file_path,stopwords)\n",
        "        else:\n",
        "            neg=  extract_words(file_path,stopwords)\n",
        "\n",
        "else:\n",
        "    print(f\"Folder not found: {folder_name}\")"
      ],
      "metadata": {
        "id": "a1FGU4GyPZ4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(neg)"
      ],
      "metadata": {
        "id": "aHAzQSELPZ9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pos)"
      ],
      "metadata": {
        "id": "cE2Qj6QgTMvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('stopwords.txt', 'r', encoding='utf-8') as stopwords_file:\n",
        "        stopwords = [line.strip() for line in stopwords_file]\n",
        "\n",
        "        positive_words = []\n",
        "        negative_words = []\n",
        "\n",
        "for file_name in files:\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        if file_name[:8] == \"positive\":\n",
        "            positive_words.extend([word for word in extract_words(file_path, stopwords) if word not in stopwords])\n",
        "        else:\n",
        "            negative_words.extend([word for word in extract_words(file_path, stopwords) if word not in stopwords])\n",
        "word_dictionary = {\n",
        "        \"positive\": positive_words,\n",
        "        \"negative\": negative_words\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "DCLOzCakNYux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2zYvFGPN-de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/negative.txt\"\n",
        "with open(file_path, 'w') as file:\n",
        "    for word in word_dictionary[\"negative\"]:\n",
        "        file.write(word + '\\n')\n",
        "\n",
        "print(\"negative list saved to:\", file_path)"
      ],
      "metadata": {
        "id": "WA19wHBtVNps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_dictionary[\"negative\"])"
      ],
      "metadata": {
        "id": "_tHFmpuIPc_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/positive.txt\"\n",
        "with open(file_path, 'w') as file:\n",
        "    for word in word_dictionary[\"positive\"]:\n",
        "        file.write(word + '\\n')\n",
        "\n",
        "print(\"positive list saved to:\", file_path)"
      ],
      "metadata": {
        "id": "XmyraBdcV43x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_dictionary[\"positive\"])"
      ],
      "metadata": {
        "id": "S3ImKTyBXN5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "WYtXkN3tPaAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(path):\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    lines = open(path, \"r\", encoding= \"utf-8\").readlines()\n",
        "    tokens = []\n",
        "    num_sen = 0\n",
        "    for line in lines:\n",
        "        sentences = line.split(\".\")\n",
        "        num_sen += len(sentences)\n",
        "        for s in sentences:\n",
        "            tokens += word_tokenize(s)\n",
        "    print(f\"tokenized {path} successfully.\")\n",
        "    return tokens, num_sen\n"
      ],
      "metadata": {
        "id": "YttMp1sKn_Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens,s = tokenizer(\"output.txt\")\n",
        "print(tokens, s)"
      ],
      "metadata": {
        "id": "IHqMLxamm6kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in tokens:\n",
        "    if token in stopwords:\n",
        "        tokens.remove(token)\n",
        "len(tokens)"
      ],
      "metadata": {
        "id": "bXVsYtWCnJtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "id": "A_0jzFWqv2pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/Input.xlsx - Sheet1.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "VinujMZBRdzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scrapearticle(\"blackassign0001\", url)\n",
        "#Article extracted and  blackassign0001 .txt created successfully"
      ],
      "metadata": {
        "id": "c2VCCcekv3tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def read_urls_from_csv(csv_file):\n",
        "    urls_df = pd.read_csv(csv_file)\n",
        "    return list(urls_df.itertuples(index=False, name=None))\n",
        "\n",
        "def scrape_articles(urls):\n",
        "    scraped_articles = []\n",
        "    for url_id, url in urls:\n",
        "        scraped_data = scrapearticle(url_id, url)\n",
        "        if scraped_data:\n",
        "            scraped_articles.append({\"Scraped Article\": scraped_data[0], \"URL\": scraped_data[1]})\n",
        "    return scraped_articles\n",
        "\n",
        "def save_to_csv(dataframe, csv_file):\n",
        "    dataframe.to_csv(csv_file, index=False)\n",
        "\n",
        "\n",
        "urls = read_urls_from_csv(\"/content/Input.xlsx - Sheet1.csv\")\n",
        "scraped_articles = scrape_articles(urls)\n",
        "df = pd.DataFrame(scraped_articles)\n",
        "save_to_csv(df, \"/content/scraped_articles.csv\")\n"
      ],
      "metadata": {
        "id": "DLJ4Ynoa9k2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_f=pd.read_csv(\"/content/scraped_articles.csv\")\n",
        "d_f"
      ],
      "metadata": {
        "id": "FWQI12sKCH2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "with open('stopwords.txt', 'r') as file:\n",
        "    stopwords = file.read().split()\n",
        "\n",
        "def clean_article(article_text):\n",
        "    words = article_text.split()\n",
        "    clean_words = [word for word in words if word.lower() not in stopwords]\n",
        "    clean_article_text = ' '.join(clean_words)\n",
        "    return clean_article_text\n",
        "\n",
        "with open('scraped_articles.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    with open('cleaned_articles.csv', 'w', newline='', encoding='utf-8') as cleaned_file:\n",
        "        fieldnames = ['URL', 'Cleaned_Article']\n",
        "        writer = csv.DictWriter(cleaned_file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for row in reader:\n",
        "            cleaned_article_text = clean_article(row['Scraped Article'])\n",
        "            writer.writerow({'URL': row['URL'], 'Cleaned_Article': cleaned_article_text})"
      ],
      "metadata": {
        "id": "YWy44JsNizJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2BxRUXSL_ndZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f=pd.read_csv(\"cleaned_articles.csv\")"
      ],
      "metadata": {
        "id": "IphUM8uQPjwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f.head(10)"
      ],
      "metadata": {
        "id": "5P3PPdsg4eI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(nltk.sent_tokenize(article_text)))"
      ],
      "metadata": {
        "id": "SEZzUj-IwBO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "punctuation_list = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '\" \"', '-']\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    for punctuation in punctuation_list:\n",
        "        text = text.replace(punctuation, '')\n",
        "    words = text.split()\n",
        "    cleaned_words = [word for word in words if word.lower() not in stopwords]\n",
        "    cleaned_text = ' '.join(cleaned_words)\n",
        "    return cleaned_text\n",
        "\n",
        "def load_word_lists():\n",
        "    with open('positive.txt', 'r') as f:\n",
        "        positive_words = set(word.strip() for word in f)\n",
        "    with open('negative.txt', 'r') as f:\n",
        "        negative_words = set(word.strip() for word in f)\n",
        "    return positive_words, negative_words\n",
        "\n",
        "def calculate_word_count(text):\n",
        "    return len(text.split())\n",
        "\n",
        "def calculate_avg_words_per_sentence(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    total_words = 0\n",
        "    total_sentences = len(sentences)\n",
        "    print(text)\n",
        "    print(sentences)\n",
        "    print(total_sentences)\n",
        "    for sentence in sentences:\n",
        "        words_in_sentence = sentence.split()\n",
        "        total_words += len(words_in_sentence)\n",
        "    avg_words_per_sentence = total_words / total_sentences if total_sentences > 0 else 0\n",
        "    return avg_words_per_sentence\n",
        "\n",
        "def calc_metrics(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    average_sentence_length = len(words) / len(sentences) if len(sentences) > 0 else 0\n",
        "    complex_word_count = sum(1 for word in words if len(word) > 6)\n",
        "    percentage_complex_words = complex_word_count / len(words) * 100 if len(words) > 0 else 0\n",
        "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
        "    return average_sentence_length, percentage_complex_words, fog_index\n",
        "\n",
        "def syllable_mapper(text):\n",
        "    mapp = {}\n",
        "    vowel = ['a', 'e', 'i', 'o', 'u']\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    for token in tokens:\n",
        "        if token not in mapp:\n",
        "            cnt = 0\n",
        "            for i in token:\n",
        "                if i.lower() in vowel:\n",
        "                    cnt += 1\n",
        "            if len(token) >= 2:\n",
        "                if token[-2:] == 'es' or token[-2:] == 'ed':\n",
        "                    cnt -= 1\n",
        "            mapp[token] = cnt\n",
        "    return mapp\n",
        "\n",
        "def count_personal_pronouns(text):\n",
        "    personal_pronouns = ['i', 'we', 'my', 'ours', 'us']\n",
        "    pattern = r'\\b(?:{})\\b'.format('|'.join(personal_pronouns))\n",
        "    matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
        "    count = len(matches)\n",
        "    return count\n",
        "\n",
        "def calculate_avg_word_length(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    total_characters = sum(len(word) for word in words)\n",
        "    total_words = len(words)\n",
        "    avg_word_length = total_characters / total_words if total_words > 0 else 0\n",
        "    return avg_word_length\n",
        "\n",
        "def count_complex_words(tokens, syllable_counts):\n",
        "    complex_word_count = sum(1 for token in tokens if syllable_counts[token] > 2)\n",
        "    return complex_word_count\n",
        "\n",
        "\n",
        "def calc_pos_score(text, positive_words):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    pos_score = sum(1 for word in words if word.lower() in positive_words)\n",
        "    return pos_score\n",
        "\n",
        "\n",
        "def calc_neg_score(text, negative_words):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    neg_score = sum(1 for word in words if word.lower() in negative_words)\n",
        "    return neg_score\n",
        "\n",
        "def calc_pol_score(pos_score, neg_score):\n",
        "    pol_score = (pos_score - neg_score) / ((pos_score + neg_score) + 0.000001)\n",
        "    return pol_score\n",
        "\n",
        "\n",
        "def calc_sub_score(pos_score, neg_score, total_words):\n",
        "    sub_score = (pos_score + neg_score) / (total_words + 0.000001)\n",
        "    return sub_score\n",
        "\n"
      ],
      "metadata": {
        "id": "q_5iTX1y_ot8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pi9PmNwIxgbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_csv(csv_file):\n",
        "    positive_words, negative_words = load_word_lists()\n",
        "    cleaned_rows = []\n",
        "    with open(csv_file, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            cleaned_article_text = clean_text(row['Cleaned_Article'])\n",
        "            row['Transformed_Text'] = cleaned_article_text\n",
        "            word_count = calculate_word_count(cleaned_article_text)\n",
        "            row['Word_Count'] = word_count\n",
        "            avg_words_per_sentence = calculate_avg_words_per_sentence(cleaned_article_text)\n",
        "            row['Avg_Word_Per_Sentence'] = avg_words_per_sentence\n",
        "            metrics = calc_metrics(cleaned_article_text)\n",
        "            row['Avg_Sentence_Length'], row['Percentage_Complex_Words'], row['Fog_Index'] = metrics\n",
        "            syllable_counts = syllable_mapper(cleaned_article_text)\n",
        "            tokens = nltk.word_tokenize(cleaned_article_text)\n",
        "            personal_pronouns_count = count_personal_pronouns(cleaned_article_text)\n",
        "            row['Personal_Pronouns_Count'] = personal_pronouns_count\n",
        "            avg_word_length = calculate_avg_word_length(cleaned_article_text)\n",
        "            row['Avg_Word_Length'] = avg_word_length\n",
        "            complex_word_count = count_complex_words(tokens, syllable_counts)\n",
        "            row['Complex_Word_Count'] = complex_word_count\n",
        "            pos_score = calc_pos_score(cleaned_article_text, positive_words)\n",
        "            row['Positive_Score'] = pos_score\n",
        "            neg_score = calc_neg_score(cleaned_article_text, negative_words)\n",
        "            row['Negative_Score'] = neg_score\n",
        "            pol_score = calc_pol_score(pos_score, neg_score)\n",
        "            row['Polarity_Score'] = pol_score\n",
        "            sub_score = calc_sub_score(pos_score, neg_score, word_count)\n",
        "            row['Subjectivity_Score'] = sub_score\n",
        "            cleaned_rows.append(row)\n",
        "\n",
        "    with open(csv_file, 'w', newline='', encoding='utf-8') as cleaned_file:\n",
        "        fieldnames = ['URL', 'Cleaned_Article', 'Transformed_Text', 'Word_Count', 'Avg_Word_Per_Sentence',\n",
        "                      'Avg_Sentence_Length', 'Percentage_Complex_Words', 'Fog_Index',\n",
        "                      'Personal_Pronouns_Count', 'Avg_Word_Length', 'Complex_Word_Count',\n",
        "                      'Positive_Score', 'Negative_Score', 'Polarity_Score', 'Subjectivity_Score']\n",
        "        writer = csv.DictWriter(cleaned_file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(cleaned_rows)\n",
        "\n",
        "\n",
        "cleaned_csv_file = '/content/cleaned_articles.csv'\n",
        "clean_csv(cleaned_csv_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "vluKvo_sICK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df=pd.read_csv(\"/content/cleaned_articles.csv\")"
      ],
      "metadata": {
        "id": "UKPJZ-lPD8ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head(5)"
      ],
      "metadata": {
        "id": "TXEc9VUyDV2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_file_name = \"Output Data Structure.xlsx\"\n",
        "new_df.to_excel(final_file_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "hhFnalLh_z7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-pi9Jq-3BqLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "48LyRoJMVlRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NcQRWexxNong"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4OXj0UQsDdiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "SOhgxm9TPC3U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}